{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Smple RAG: Document Retrieval and Text Generation**\n",
    "\n",
    "In the age of information overload, having tools that can efficiently retrieve, process, and generate insights is more valuable than ever. This blog will guide you through building a **Retrieval-Augmented Generation (RAG)** pipeline. With just a few lines of Python, you’ll be able to transform complex documents into actionable knowledge. Let’s get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Step 0: Setting Up the Essentials**\n",
    "\n",
    "Before we dive into the pipeline, let’s load the necessary libraries. These tools form the backbone of our RAG framework: \n",
    "\n",
    "\n",
    "\n",
    "- **LangChain Libraries**: These power everything from document loading and splitting to embedding storage and output parsing.  \n",
    "- **PyMuPDFLoader**: Loads PDF documents into processable chunks.  \n",
    "- **Chroma**: Manages the vector storage for semantic search.\n",
    "- **Langsmith**: This is used for monitoring the calls and easy to debug.  \n",
    "- Also load the api key from the .env file for embeddings\n",
    "\n",
    "And of course, the **pprint** module ensures outputs are beautifully formatted for debugging and exploration.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import pprint\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv('D:/Code/AI/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGSMITH_PROJECT']=\"RAG_Simple\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Step 1: Indexing Documents**\n",
    "\n",
    "### **1.1 Loading Documents**\n",
    "First, let’s load our document. Imagine you have a PDF with key insights—this is where `PyMuPDFLoader` comes into play.  \n",
    "\n",
    "\n",
    "- The `PyMuPDFLoader` processes the document lazily (page by page), saving memory when dealing with large files.  \n",
    "- Each page is appended to a list called `page`, preparing it for text splitting.  \n",
    "\n",
    "**Why it matters**: Efficient loading ensures scalability for massive documents like research papers or legal contracts.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"short.pdf\") # load any pdf file which you have.\n",
    "\n",
    "page = []\n",
    "for doc in loader.lazy_load():\n",
    "    page.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.2 Splitting the Text**\n",
    "To manage large blocks of text effectively, we split them into smaller, manageable chunks using a **Recursive Character Text Splitter**.\n",
    "\n",
    "\n",
    "- **Chunk Size**: Each chunk contains up to 1024 characters.  \n",
    "- **Overlap**: A 200-character overlap between chunks ensures that no context is lost.  \n",
    "\n",
    "This intelligent splitting method avoids breaking meaningful sentences or paragraphs, resulting in cleaner and more cohesive data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.3 Creating Semantic Embeddings**\n",
    "Once we have our text chunks, it’s time to convert them into **embeddings**—numerical representations of text.\n",
    "\n",
    "\n",
    "\n",
    "Here’s what’s happening:  \n",
    "- **Embeddings**: The `GoogleGenerativeAIEmbeddings` model encodes text into high-dimensional vectors that capture semantic meaning.  \n",
    "- **Vector Store**: Chroma stores these embeddings efficiently for quick retrieval.  \n",
    "- **Retriever**: Acts as a bridge, finding the most relevant chunks for any given query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Retrieval and Text Generation**\n",
    "\n",
    "### **2.1 Loading a Prompt**\n",
    "The system needs a blueprint to guide how it generates responses. That’s where a **prompt** comes in.  \n",
    "\n",
    "\n",
    "\n",
    "This prompt serves as a structured template, helping the language model generate coherent and useful answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are an assistant for question-answering tasks. Use the following pieces '\n",
      " \"of retrieved context to answer the question. If you don't know the answer, \"\n",
      " \"just say that you don't know. Use three sentences maximum and keep the \"\n",
      " 'answer concise.\\n'\n",
      " 'Question: {question} \\n'\n",
      " 'Context: {context} \\n'\n",
      " 'Answer:')\n"
     ]
    }
   ],
   "source": [
    "# Pre-built tempate present in the Langchain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "pprint.pprint(prompt[0].prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Configuring the Language Model**\n",
    "Next, we initialize our Large Language Model (LLM). In this example, we use **ChatOllama**, a 14-billion-parameter powerhouse.  \n",
    "\n",
    "\n",
    "\n",
    "This model excels at understanding context and generating human-like responses, making it a great fit for RAG pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using offline model \n",
    "llm = ChatOllama(model=\"deepseek-r1:14b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Formatting Retrieved Documents**\n",
    "Before feeding documents to the LLM, we clean and format the retrieved text.  \n",
    "\n",
    "\n",
    "\n",
    "This function combines chunks into a single, cohesive string, ensuring the model processes them smoothly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Building the RAG Chain**\n",
    "Finally, we bring all the components together into a seamless **RAG chain**.\n",
    "\n",
    "Here’s how it works:  \n",
    "1. **Retriever**: Fetches the most relevant chunks.  \n",
    "2. **Formatting**: Combines chunks into a clean context.  \n",
    "3. **Prompt**: Structures the input for the LLM.  \n",
    "4. **LLM**: Generates an insightful response.  \n",
    "5. **Output Parsing**: Converts the response into a human-readable format.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How It All Comes Together**\n",
    "\n",
    "Imagine you have a PDF about pharmaceutical regulations, and you need answers to specific questions:  \n",
    "1. **Input your query** into the RAG pipeline.  \n",
    "2. **Retrieve relevant chunks** from the document.  \n",
    "3. **Feed the context** and query to the LLM.  \n",
    "4. **Generate a detailed response** based on your document.  \n",
    "\n",
    "This pipeline transforms a static PDF into a dynamic, interactive knowledge source!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<think>\\n'\n",
      " \"Okay, so the user is asking about Gabriel García Márquez's masterpieces and \"\n",
      " 'whether he won any prizes. Let me start by looking through the context '\n",
      " 'provided.\\n'\n",
      " '\\n'\n",
      " 'First, I see that the context mentions he was a novelist, short-story '\n",
      " 'writer, and journalist. It also states that he is considered one of the '\n",
      " 'greatest Latin American masters of narrative. That’s a good sign he has some '\n",
      " 'notable works.\\n'\n",
      " '\\n'\n",
      " 'Looking further down, it explicitly says his two masterpieces are \"One '\n",
      " 'Hundred Years of Solitude\" from 1967 and \"Love in the Time of Cholera\" from '\n",
      " '1985. So that answers the first part about his masterpieces.\\n'\n",
      " '\\n'\n",
      " 'Then, the context mentions he won the Nobel Prize in Literature in 1982. '\n",
      " 'That directly answers the second part of the question regarding any prizes '\n",
      " 'he received.\\n'\n",
      " '\\n'\n",
      " 'I should make sure to keep the answer concise, using three sentences maximum '\n",
      " 'as instructed. I need to include both the masterpieces and mention the Nobel '\n",
      " 'Prize win.\\n'\n",
      " '\\n'\n",
      " 'Wait, is there any other information? The context also talks about his '\n",
      " 'themes like violence, solitude, and human need for love, but since the '\n",
      " \"question doesn't ask about that, I can omit it for brevity.\\n\"\n",
      " '\\n'\n",
      " \"I think that's all. So, the answer will list the two masterpieces and state \"\n",
      " 'he won the Nobel Prize in 1982.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'Gabriel García Márquez\\'s masterpieces include \"One Hundred Years of '\n",
      " 'Solitude\" (1967) and \"Love in the Time of Cholera\" (1985). He was awarded '\n",
      " 'the Nobel Prize in Literature in 1982.')\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "response=rag_chain.invoke(\"what are Garcia Marquez masterpieces and did he get any prize\")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applications**\n",
    "- **Healthcare**: Extract insights from medical research papers.  \n",
    "- **Legal**: Quickly answer questions based on contracts or case files.  \n",
    "- **Customer Support**: Build smarter chatbots with real-time, document-based answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Final Thoughts**\n",
    "By combining document retrieval, embedding storage, and LLMs, this RAG pipeline is a game-changer for handling complex information. Whether you’re in pharma, legal, or tech, this system simplifies the process of turning data into actionable knowledge.\n",
    "\n",
    "With minimal code and maximum flexibility, building your own RAG system has never been easier. So, what are you waiting for? Dive into your documents and start generating insights today! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
