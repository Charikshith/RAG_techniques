## RAG Must-Know Terms

Authored by Charikshith. To stay updated with LLM, RAG and Agent updates, you can follow me on [LinkedIn](Your LinkedIn Profile Link) and [YouTube](Your YouTube Channel Link).

Here are the must-know RAG terms, categorized for clarity:

### General RAG Terms

*   **Query** - The input text or question provided by a user to retrieve relevant information. It guides the retrieval and generation process in the RAG system.

*   **Chunk** – A small segment of a larger document, typically a paragraph or sentence. Chunking helps improve retrieval efficiency and relevance in RAG.

*   **Chunking** – The process of breaking down large text into smaller, meaningful segments. This ensures efficient storage, retrieval, and contextual relevance.

*   **Retrieval-Augmented Generation (RAG)** - The end-to-end process of enhancing LLM responses by first retrieving relevant information from an external knowledge source and then using that information to generate a more informed and accurate response.

### RAG Indexing (Preparation of Knowledge Base)

*   **Embedding** – A numerical representation of text that captures semantic meaning. It enables efficient similarity comparisons in vector spaces.

*   **Indexing** – Organizing and storing embeddings in a structured manner for fast retrieval. It helps locate relevant information efficiently in large datasets.

*   **Vector Store** (or **Vector Database**) – A specialized database that stores text embeddings as vectors. It enables fast and accurate similarity-based retrieval.  Examples include Pinecone, Weaviate, Chroma, FAISS, and Annoy.

*   **Embedding Model** -  A machine learning model used to convert text into vector embeddings. Examples include Sentence Transformers, OpenAI Embeddings (text-embedding-ada-002), and Cohere Embeddings. The choice of embedding model significantly impacts retrieval quality.

### RAG Retrieval (Finding Relevant Information)

*   **Retrieval** – The process of fetching relevant chunks of information from the vector store based on a query. It is a key step in augmenting LLMs with external knowledge.

*   **Semantic Search** – A search technique that finds results based on meaning rather than exact words. It uses embeddings to capture contextual similarities and find conceptually related information.

*   **Keyword Search** – A traditional search method that matches exact words or phrases in documents to a query. It does not consider semantic meaning, only textual occurrence.

*   **Hybrid Search** – A combination of keyword search and semantic search. It balances precision and recall by leveraging both techniques to improve retrieval effectiveness.

*   **Relevance Score** - A numerical value indicating how similar or relevant a retrieved chunk is to the query.  Calculated based on similarity metrics in the vector space (e.g., cosine similarity).

### RAG Augmentation (Preparing the Prompt)

*   **Augmentation** – Enhancing the input query with retrieved information (context) before passing it to an LLM. It provides additional context to improve response quality and factuality.

*   **Context** – The surrounding information that helps an LLM generate relevant and grounded responses. In RAG, it is obtained by merging the retrieved relevant chunks.

*   **Context Window** – The maximum number of tokens (words or sub-word units) an LLM can process at once in a single input. RAG must ensure the prompt (query + context) fits within this limit.

*   **Prompt** – The formatted input given to an LLM to generate a response. In RAG, it typically includes the user query, retrieved context, and specific instructions for the LLM.

*   **Prompt Engineering** – The practice of designing effective prompts to guide LLM responses. It optimizes clarity, structure, and intent for better outputs, and in RAG, includes strategies for incorporating retrieved context effectively.

### RAG Generation (LLM Response)

*   **Generation** – The process where an LLM produces text based on a given prompt. It synthesizes information from its pre-trained knowledge and the provided context to generate coherent and relevant responses.

*   **Large Language Model (LLM)** – A deep learning model (typically a transformer network) trained on vast amounts of text data to understand and generate human-like language. It powers RAG systems by interpreting queries, utilizing context, and generating the final response. Examples include GPT-4, Claude, Llama 2, and Gemini.

*   **Response** – The output generated by an LLM in answer to a query. In RAG, it is ideally a factually accurate, contextually relevant, and helpful answer grounded in the retrieved information.

### LLM Generation Parameters (Controlling Output)

*   **Temperature** – A parameter that controls randomness in an LLM’s output during generation. Higher temperature values (e.g., 0.7-1.0) produce more diverse and creative responses, while lower values (e.g., 0.0-0.3) make them more deterministic and focused.

*   **Top-K (or Top-P) Sampling** - Parameters that control the token selection process during generation, influencing the diversity and coherence of the output. They limit the LLM's choices to the most probable tokens, impacting the generated text style.

*   **Max Tokens (or Max Length)** -  A parameter that limits the maximum length (in tokens) of the generated response from the LLM.  Important for controlling output length and resource usage.

---

**Additional Terms to Consider (Depending on Depth):**

*   **Knowledge Base** - The external source of information used in RAG. Can be a vector database, document store, knowledge graph, web, or any collection of data.

*   **Data Source** - The origin of the data used to build the knowledge base (e.g., website, internal documents, APIs).

*   **Evaluation Metrics (for RAG)** - Metrics used to assess RAG system performance, such as:
    *   **Context Relevance**: How relevant the retrieved context is to the query.
    *   **Faithfulness / Groundedness**: How well the generated response is supported by the retrieved context (avoids hallucination).
    *   **Answer Relevance**: How well the generated answer addresses the user's query.

*   **RAG Pipeline** - The complete sequence of steps in a RAG system, from indexing to generation.

*   **Basic RAG vs. Advanced RAG** - Categorization of RAG techniques. Basic RAG is the fundamental approach described above. Advanced RAG includes techniques like query expansion, re-ranking, context compression, and more sophisticated retrieval and generation strategies.

*   **Query Expansion** - Techniques to reformulate or expand the user query to improve retrieval effectiveness (e.g., adding synonyms, related terms).

*   **Context Compression** - Methods to reduce the size of the retrieved context while preserving the most important information, to fit within the LLM's context window and improve efficiency.

*   **Re-ranking** -  Applying a second-stage ranking model to re-order the initially retrieved chunks, further improving the relevance of the context provided to the LLM.

This list covers the essential RAG terms. Depending on your audience and the level of detail required, you might consider adding some of the "Additional Terms" for a more comprehensive glossary. Remember to replace the placeholder links with your actual LinkedIn, Twitter, and YouTube profiles!
